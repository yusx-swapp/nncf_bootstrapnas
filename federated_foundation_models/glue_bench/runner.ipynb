{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments,\n",
    "    AutoTokenizer,PreTrainedTokenizerFast,\n",
    "    AutoModel,\n",
    "    BertForSequenceClassification,\n",
    "    )\n",
    "from transformers import RobertaTokenizerFast, T5Tokenizer\n",
    "from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification, T5ForConditionalGeneration\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import logging\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-large-uncased-whole-word-masking'\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# The identifier name of the pre-trained model\n",
    "model_name = 'bert-large-uncased-whole-word-masking'\n",
    "\n",
    "# Load the configuration from the identifier\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# The config object now contains the configuration of the BERT model\n",
    "config.num_labels = 2\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# model = AutoModel.from_pretrained(model_name, config=config)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# kd_teacher_model = copy.deepcopy(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "   model_name,\n",
    "   use_fast=True,\n",
    ")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    raise ValueError(\n",
    "        \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n",
    "        \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n",
    "        \" this requirement\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing training data\n",
    "### 2.1 Define a tikenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer, dataset,model=None):\n",
    "    if dataset in [\"sst2\", \"cola\"]:\n",
    "\n",
    "        return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "    elif dataset == \"mnli\":\n",
    "        return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "    elif dataset == \"qqp\":\n",
    "        return tokenizer(examples[\"question1\"], examples[\"question2\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "    elif dataset == \"qnli\":\n",
    "        return tokenizer(examples[\"question\"], examples[\"sentence\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "    elif dataset in [\"mrpc\", \"stsb\", \"rte\"]:\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'sst2'\n",
    "dataset = load_dataset(\"glue\", dataset_name)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(lambda examples: tokenize_function(examples, tokenizer, dataset_name), batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(lambda examples: tokenize_function(examples, tokenizer, dataset_name), batched=True)\n",
    "# logging.info(\"=====> train_dataset size: {}\".format(len(tokenized_train_dataset)))\n",
    "print(\"=====> train_dataset size: {}\".format(len(tokenized_train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start training the original BERT Model\n",
    "### 3.1 Define training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./log_dir',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./log_dir',\n",
    "    save_total_limit = 1,\n",
    "    save_strategy = \"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, task):\n",
    "    predictions, labels = eval_pred\n",
    "    if task == \"stsb\":\n",
    "        pearson_corr, _ = pearsonr(predictions.squeeze(), labels)\n",
    "        return {\"pearson_corr\": pearson_corr}\n",
    "    else:\n",
    "    \n",
    "        predictions = predictions.argmax(-1)\n",
    "        return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Start training using trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_test_dataset,  # Pass tokenized_test_dataset instead of test_dataset\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, dataset_name),\n",
    "        # kd_teacher_model = kd_teacher_model\n",
    "    )\n",
    "\n",
    "\n",
    "    #get the best model\n",
    "    \n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nncf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
