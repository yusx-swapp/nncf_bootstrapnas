{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments,\n",
    "    AutoTokenizer,PreTrainedTokenizerFast,\n",
    "    AutoModel,\n",
    "    BertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    )\n",
    "from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm\n",
    "\n",
    "from trainer_supernet import SupernetTrainer\n",
    "from transformers import RobertaTokenizerFast, T5Tokenizer\n",
    "from transformers import DistilBertForSequenceClassification, RobertaForSequenceClassification, T5ForConditionalGeneration\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import logging\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "from nncf import NNCFConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Create Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=384,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when\"\n",
    "                \" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    version_2_with_negative: bool = field(\n",
    "        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\n",
    "    )\n",
    "    null_score_diff_threshold: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n",
    "                \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n",
    "                \"Only useful when `version_2_with_negative=True`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    doc_stride: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\n",
    "    )\n",
    "    n_best_size: int = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n",
    "    )\n",
    "    max_answer_length: int = field(\n",
    "        default=30,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum length of an answer that can be generated. This is needed because the start \"\n",
    "                \"and end predictions are not conditioned on one another.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if (\n",
    "            self.dataset_name is None\n",
    "            and self.train_file is None\n",
    "            and self.validation_file is None\n",
    "            and self.test_file is None\n",
    "        ):\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file/test_file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "            if self.test_file is not None:\n",
    "                extension = self.test_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing training data\n",
    "### 1.1 Define a tikenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer, dataset,model=None):\n",
    "    if dataset in [\"sst2\", \"cola\"]:\n",
    "\n",
    "        return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "    elif dataset == \"mnli\":\n",
    "        return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "    elif dataset == \"qqp\":\n",
    "        return tokenizer(examples[\"question1\"], examples[\"question2\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "    elif dataset == \"qnli\":\n",
    "        return tokenizer(examples[\"question\"], examples[\"sentence\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "    elif dataset in [\"mrpc\", \"stsb\", \"rte\"]:\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True,return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/homes/yusx/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 370.18it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'sst2'\n",
    "    \n",
    "num_classes = {\n",
    "    \"mnli\": 3,\n",
    "    \"qqp\": 2,\n",
    "    \"qnli\": 2,\n",
    "    \"sst2\": 2,\n",
    "    \"stsb\": 1,\n",
    "    \"mrpc\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"cola\": 2,\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"glue\", dataset_name)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model & Tokenizer\n",
    "### 2.1 Get model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.29.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-large-uncased-whole-word-masking'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config.num_labels = num_classes[dataset_name]\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForQuestionAnswering\n",
    "# model_qa = AutoModelForQuestionAnswering.from_pretrained(model_name, config = config)\n",
    "# print(model_qa)\n",
    "# model_qa = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# kd_teacher_model = copy.deepcopy(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-large-uncased-whole-word-masking', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "   model_name,\n",
    "   use_fast=True,\n",
    ")\n",
    "print(tokenizer)\n",
    "if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    raise ValueError(\n",
    "        \"This example script only works for models that have a fast tokenizer. Checkout the big table of models at\"\n",
    "        \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n",
    "        \" this requirement\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /homes/yusx/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a4ff0bfa4db56cf0.arrow\n",
      "Loading cached processed dataset at /homes/yusx/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-19ce2cdb4180a76a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> train_dataset size: 67349\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(lambda examples: tokenize_function(examples, tokenizer, dataset_name), batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(lambda examples: tokenize_function(examples, tokenizer, dataset_name), batched=True)\n",
    "# logging.info(\"=====> train_dataset size: {}\".format(len(tokenized_train_dataset)))\n",
    "print(\"=====> train_dataset size: {}\".format(len(tokenized_train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, task):\n",
    "    predictions, labels = eval_pred\n",
    "    if task == \"stsb\":\n",
    "        pearson_corr, _ = pearsonr(predictions.squeeze(), labels)\n",
    "        return {\"pearson_corr\": pearson_corr}\n",
    "    else:\n",
    "    \n",
    "        predictions = predictions.argmax(-1)\n",
    "        return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create NNCF Supernet from Base Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually setup some args to align with the arguments in BoostrapNAS demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-large-uncased-whole-word-masking'\n",
    "\n",
    "command_line_args = [\"--model_name_or_path\", model_name,\\\n",
    "                     \"--kd_teacher_model\", model_name,\\\n",
    "                      \"--do_train\",\\\n",
    "                     \"--dataset_name\",\"squad\", \"--output_dir\",\"./log_dir\"]\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(command_line_args)\n",
    "# Manually setup some args to align with the arguments in BoostrapNAS demo\n",
    "training_args.__setattr__('nncf_config','./nncf_config.json')\n",
    "training_args.__setattr__('do_train',True)\n",
    "training_args.__setattr__('do_eval',True)\n",
    "training_args.__setattr__('output_dir','./log_dir/')\n",
    "# Manually setup some args to align with the arguments in BoostrapNAS demo\n",
    "model_args.__setattr__('model_revision','model_revision')\n",
    "model_args.__setattr__('use_auth_token',False)\n",
    "model_args.__setattr__('model_name_or_path',model_name)\n",
    "print(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "kd_teacher_model = None\n",
    "if training_args.kd_teacher_model:\n",
    "    kd_teacher_model = BertForSequenceClassification.from_pretrained(\n",
    "        training_args.kd_teacher_model,\n",
    "        from_tf=bool(\".ckpt\" in training_args.kd_teacher_model),\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    kd_teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data collator\n",
    "# # We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# # collator.\n",
    "# data_collator = (\n",
    "#     default_data_collator\n",
    "#     if data_args.pad_to_max_length\n",
    "#     else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "nncf_config = None\n",
    "create_importance_mask_fn = None\n",
    "if training_args.nncf_config is not None:\n",
    "    nncf_config = NNCFConfig.from_json(training_args.nncf_config)\n",
    "    if nncf_config.get(\"log_dir\") is None:\n",
    "        nncf_config[\"log_dir\"] = training_args.output_dir\n",
    "    if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:\n",
    "        os.makedirs(nncf_config[\"log_dir\"])\n",
    "\n",
    "    # split nncf_config --> bnas + movement sparsity\n",
    "    compression_algo_names = [algo['algorithm'] for algo in nncf_config.get('compression', [])]\n",
    "    if 'movement_sparsity' in compression_algo_names:\n",
    "        sparsity_nncf_config = copy.deepcopy(nncf_config)\n",
    "        if 'bootstrapNAS' in sparsity_nncf_config:\n",
    "            del sparsity_nncf_config['bootstrapNAS']\n",
    "\n",
    "        def generate_importance_mask_weight(model_state_dict, debug_mode=False, save_folder='movement_sparsity',\n",
    "                                            resume_model='/data2/yzheng/bert_squad_result/weight_reorg/checkpoint-29509/pytorch_model.bin'):\n",
    "            # create movement sparsity ctrl\n",
    "            sparsity_ctrl, sparsity_model = BertForSequenceClassification.from_pretrained(\n",
    "                model_name,\n",
    "                from_tf=bool(\".ckpt\" in model_name),\n",
    "                config=config,\n",
    "                # cache_dir=model_args.cache_dir,\n",
    "                revision=model_args.model_revision,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "                nncf_config=sparsity_nncf_config,\n",
    "                nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train,\n",
    "            )\n",
    "            sparsity_model.load_state_dict(model_state_dict, strict=False) # use current bnas model state dict\n",
    "\n",
    "            sparsity_trainer = SupernetTrainer(\n",
    "                model=sparsity_model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_train_dataset if training_args.do_train else None,\n",
    "                eval_dataset=tokenized_train_dataset if training_args.do_eval else None,\n",
    "                eval_examples=test_dataset if training_args.do_eval else None,\n",
    "                tokenizer=tokenizer,\n",
    "                # data_collator=data_collator,\n",
    "                # post_process_function=post_processing_function,\n",
    "                compute_metrics=lambda eval_pred: compute_metrics(eval_pred, dataset_name),\n",
    "                compression_ctrl=sparsity_ctrl,\n",
    "                kd_teacher_model=kd_teacher_model,\n",
    "                debug_mode=debug_mode,\n",
    "                resume_model=resume_model\n",
    "            )\n",
    "\n",
    "            sparsity_trainer.train()\n",
    "            sparsity_trainer.save_model(os.path.join(training_args.output_dir, save_folder))\n",
    "            return sparsity_model\n",
    "\n",
    "        create_importance_mask_fn = generate_importance_mask_weight\n",
    "\n",
    "retval = BertForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    "    nncf_config=nncf_config,\n",
    "    nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train,\n",
    "    )\n",
    "\n",
    "if nncf_config is None:\n",
    "    model = retval\n",
    "    compression_ctrl = None\n",
    "else:\n",
    "    # if movement sparsity is in config,\n",
    "    # then compression_ctrl = [bnas_ctrl, movement_sparsity_ctrl]\n",
    "    # model = [bnas_model, movement_sparsity_model]\n",
    "    compression_ctrl, model = retval\n",
    "    compression_ctrl.multi_elasticity_handler.width_handler.create_importance_mask_fn = create_importance_mask_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): NNCFEmbedding(\n",
      "        30522, 1024, padding_idx=0\n",
      "        (pre_ops): ModuleDict()\n",
      "        (post_ops): ModuleDict()\n",
      "      )\n",
      "      (position_embeddings): NNCFEmbedding(\n",
      "        512, 1024\n",
      "        (pre_ops): ModuleDict()\n",
      "        (post_ops): ModuleDict()\n",
      "      )\n",
      "      (token_type_embeddings): NNCFEmbedding(\n",
      "        2, 1024\n",
      "        (pre_ops): ModuleDict()\n",
      "        (post_ops): ModuleDict()\n",
      "      )\n",
      "      (LayerNorm): NNCFLayerNorm(\n",
      "        (1024,), eps=1e-12, elementwise_affine=True\n",
      "        (pre_ops): ModuleDict(\n",
      "          (0): UpdateLayerNormParams(\n",
      "            (op): ElasticInputWidthLayerNormOp()\n",
      "          )\n",
      "        )\n",
      "        (post_ops): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): NNCFLinear(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (pre_ops): ModuleDict(\n",
      "                  (0): UpdateWeight(\n",
      "                    (op): ElasticInputWidthLinearOp()\n",
      "                  )\n",
      "                  (1): UpdateWeightAndOptionalBias(\n",
      "                    (op): ElasticOutputWidthLinearOp()\n",
      "                  )\n",
      "                )\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (key): NNCFLinear(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (pre_ops): ModuleDict(\n",
      "                  (0): UpdateWeight(\n",
      "                    (op): ElasticInputWidthLinearOp()\n",
      "                  )\n",
      "                  (1): UpdateWeightAndOptionalBias(\n",
      "                    (op): ElasticOutputWidthLinearOp()\n",
      "                  )\n",
      "                )\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (value): NNCFLinear(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (pre_ops): ModuleDict(\n",
      "                  (0): UpdateWeight(\n",
      "                    (op): ElasticInputWidthLinearOp()\n",
      "                  )\n",
      "                  (1): UpdateWeightAndOptionalBias(\n",
      "                    (op): ElasticOutputWidthLinearOp()\n",
      "                  )\n",
      "                )\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): NNCFLinear(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (pre_ops): ModuleDict(\n",
      "                  (0): UpdateWeight(\n",
      "                    (op): ElasticInputWidthLinearOp()\n",
      "                  )\n",
      "                )\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): NNCFLayerNorm(\n",
      "                (1024,), eps=1e-12, elementwise_affine=True\n",
      "                (pre_ops): ModuleDict(\n",
      "                  (0): UpdateLayerNormParams(\n",
      "                    (op): ElasticInputWidthLayerNormOp()\n",
      "                  )\n",
      "                )\n",
      "                (post_ops): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): NNCFLinear(\n",
      "              in_features=1024, out_features=4096, bias=True\n",
      "              (pre_ops): ModuleDict(\n",
      "                (0): UpdateWeight(\n",
      "                  (op): ElasticInputWidthLinearOp()\n",
      "                )\n",
      "                (1): UpdateWeightAndOptionalBias(\n",
      "                  (op): ElasticOutputWidthLinearOp()\n",
      "                )\n",
      "              )\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): NNCFLinear(\n",
      "              in_features=4096, out_features=1024, bias=True\n",
      "              (pre_ops): ModuleDict(\n",
      "                (0): UpdateWeight(\n",
      "                  (op): ElasticInputWidthLinearOp()\n",
      "                )\n",
      "              )\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (LayerNorm): NNCFLayerNorm(\n",
      "              (1024,), eps=1e-12, elementwise_affine=True\n",
      "              (pre_ops): ModuleDict(\n",
      "                (0): UpdateLayerNormParams(\n",
      "                  (op): ElasticInputWidthLayerNormOp()\n",
      "                )\n",
      "              )\n",
      "              (post_ops): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): NNCFLinear(\n",
      "        in_features=1024, out_features=1024, bias=True\n",
      "        (pre_ops): ModuleDict(\n",
      "          (0): UpdateWeight(\n",
      "            (op): ElasticInputWidthLinearOp()\n",
      "          )\n",
      "        )\n",
      "        (post_ops): ModuleDict()\n",
      "      )\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): NNCFLinear(\n",
      "    in_features=1024, out_features=2, bias=True\n",
      "    (pre_ops): ModuleDict(\n",
      "      (0): UpdateWeight(\n",
      "        (op): ElasticInputWidthLinearOp()\n",
      "      )\n",
      "    )\n",
      "    (post_ops): ModuleDict()\n",
      "  )\n",
      "  (_nncf): NNCFNetworkInterface()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SupernetTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=tokenized_test_dataset if training_args.do_eval else None,\n",
    "    eval_examples=test_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    # post_process_function=post_processing_function,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, dataset_name),\n",
    "    compression_ctrl=compression_ctrl,\n",
    "    kd_teacher_model=kd_teacher_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Stage LR scheduler in use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/yusx/sixing/nncf_bootstrapnas/nncf/torch/nncf_network.py:982: FutureWarning: Calls to NNCFNetwork.get_nncf_wrapped_model() are deprecated and will be removed in NNCF v2.6.0.\n",
      "Starting from NNCF v2.5.0, the compressed model object already inherits the original class of the uncompressed model and the forward signature, so the call to .get_nncf_wrapped_model() may be simply omitted.\n",
      "  warning_deprecated(\n",
      "/homes/yusx/sixing/transformers/src/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use movement sparsity to generate weight importance --> weight reorg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No match has been found among the model operations for the following ignored/target scope definitions:\n - ignored_scope: ['{re}.*qa_outputs.*']\nRefer to the original_graph.dot to discover the operations in the model currently visible to NNCF and specify the ignored/target scopes in terms of the names there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39melif\u001b[39;00m last_checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     checkpoint \u001b[39m=\u001b[39m last_checkpoint\n\u001b[0;32m----> 9\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mcheckpoint)\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m nncf_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     train_result, model, elasticity_ctrl \u001b[39m=\u001b[39m train_result\n",
      "File \u001b[0;32m~/sixing/transformers/src/transformers/trainer.py:1691\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1688\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1689\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1690\u001b[0m )\n\u001b[0;32m-> 1691\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1692\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1693\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1694\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1695\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1696\u001b[0m )\n",
      "File \u001b[0;32m~/sixing/transformers/src/transformers/trainer.py:1946\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39monly to get weight importance mask, stop training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1945\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression_ctrl\u001b[39m.\u001b[39;49mscheduler\u001b[39m.\u001b[39;49mepoch_step()\n\u001b[1;32m   1947\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression_stage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression_ctrl\u001b[39m.\u001b[39mcompression_stage()\n\u001b[1;32m   1948\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression_ctrl\u001b[39m.\u001b[39mstatistics()\u001b[39m.\u001b[39mto_str())\n",
      "File \u001b[0;32m~/sixing/nncf_bootstrapnas/nncf/experimental/torch/nas/bootstrapNAS/training/scheduler.py:190\u001b[0m, in \u001b[0;36mBootstrapNASScheduler.epoch_step\u001b[0;34m(self, next_epoch)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mif\u001b[39;00m width_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mgetattr\u001b[39m(width_handler, \u001b[39m\"\u001b[39m\u001b[39mcreate_importance_mask_fn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m     width_handler\u001b[39m.\u001b[39madd_weight_importance_model(\n\u001b[0;32m--> 190\u001b[0m         width_handler\u001b[39m.\u001b[39;49mcreate_importance_mask_fn(\n\u001b[1;32m    191\u001b[0m             width_handler\u001b[39m.\u001b[39;49m_target_model\u001b[39m.\u001b[39;49mstate_dict(),\n\u001b[1;32m    192\u001b[0m             save_folder\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmovement_sparsity_\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_epoch\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    193\u001b[0m         )\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m     width_handler\u001b[39m.\u001b[39mreorganize_weights_with_importance_mask()\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mgenerate_importance_mask_weight\u001b[0;34m(model_state_dict, debug_mode, save_folder, resume_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_importance_mask_weight\u001b[39m(model_state_dict, debug_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, save_folder\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmovement_sparsity\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m                                     resume_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/data2/yzheng/bert_squad_result/weight_reorg/checkpoint-29509/pytorch_model.bin\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# create movement sparsity ctrl\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     sparsity_ctrl, sparsity_model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     21\u001b[0m         model_name,\n\u001b[1;32m     22\u001b[0m         from_tf\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39min\u001b[39;49;00m model_name),\n\u001b[1;32m     23\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     24\u001b[0m         \u001b[39m# cache_dir=model_args.cache_dir,\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m         revision\u001b[39m=\u001b[39;49mmodel_args\u001b[39m.\u001b[39;49mmodel_revision,\n\u001b[1;32m     26\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m model_args\u001b[39m.\u001b[39;49muse_auth_token \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     27\u001b[0m         nncf_config\u001b[39m=\u001b[39;49msparsity_nncf_config,\n\u001b[1;32m     28\u001b[0m         nncf_eval\u001b[39m=\u001b[39;49mnncf_config \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39mand\u001b[39;49;00m training_args\u001b[39m.\u001b[39;49mdo_eval \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m training_args\u001b[39m.\u001b[39;49mdo_train,\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     sparsity_model\u001b[39m.\u001b[39mload_state_dict(model_state_dict, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# use current bnas model state dict\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     sparsity_trainer \u001b[39m=\u001b[39m SupernetTrainer(\n\u001b[1;32m     33\u001b[0m         model\u001b[39m=\u001b[39msparsity_model,\n\u001b[1;32m     34\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m         resume_model\u001b[39m=\u001b[39mresume_model\n\u001b[1;32m     46\u001b[0m     )\n",
      "File \u001b[0;32m~/sixing/transformers/src/transformers/modeling_utils.py:2831\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2829\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnncf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m create_compressed_model\n\u001b[1;32m   2830\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39muse movement sparsity to generate weight importance --> weight reorg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 2831\u001b[0m     compression_ctrl, model \u001b[39m=\u001b[39m create_compressed_model(model, nncf_config)\n\u001b[1;32m   2833\u001b[0m     \u001b[39mreturn\u001b[39;00m compression_ctrl, model\n\u001b[1;32m   2835\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo algorithm is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sixing/nncf_bootstrapnas/nncf/telemetry/decorator.py:71\u001b[0m, in \u001b[0;36mtracked_function.__call__.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n\u001b[1;32m     64\u001b[0m         telemetry\u001b[39m.\u001b[39msend_event(\n\u001b[1;32m     65\u001b[0m             event_category\u001b[39m=\u001b[39mcategory,\n\u001b[1;32m     66\u001b[0m             event_action\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mname,\n\u001b[1;32m     67\u001b[0m             event_label\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mdata,\n\u001b[1;32m     68\u001b[0m             event_value\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mint_data,\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 71\u001b[0m retval \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m category \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m category \u001b[39m!=\u001b[39m previous_category:\n\u001b[1;32m     74\u001b[0m     telemetry\u001b[39m.\u001b[39mend_session(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category)\n",
      "File \u001b[0;32m~/sixing/nncf_bootstrapnas/nncf/torch/model_creation.py:128\u001b[0m, in \u001b[0;36mcreate_compressed_model\u001b[0;34m(model, config, compression_state, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn, dump_graphs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m is_state_loadable:\n\u001b[1;32m    127\u001b[0m     builder\u001b[39m.\u001b[39mload_state(compression_state[BaseController\u001b[39m.\u001b[39mBUILDER_STATE])\n\u001b[0;32m--> 128\u001b[0m compressed_model \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mapply_to(nncf_network)\n\u001b[1;32m    129\u001b[0m compression_ctrl \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39mbuild_controller(compressed_model)\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m is_state_loadable:\n",
      "File \u001b[0;32m~/sixing/nncf_bootstrapnas/nncf/torch/compression_method_api.py:122\u001b[0m, in \u001b[0;36mPTCompressionAlgorithmBuilder.apply_to\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_to\u001b[39m(\u001b[39mself\u001b[39m, model: NNCFNetwork) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NNCFNetwork:\n\u001b[1;32m    121\u001b[0m     transformer \u001b[39m=\u001b[39m PTModelTransformer(model)\n\u001b[0;32m--> 122\u001b[0m     transformation_layout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_transformation_layout(model)\n\u001b[1;32m    123\u001b[0m     transformed_model \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mtransform(transformation_layout)\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_init:\n",
      "File \u001b[0;32m~/sixing/nncf_bootstrapnas/nncf/torch/compression_method_api.py:139\u001b[0m, in \u001b[0;36mPTCompressionAlgorithmBuilder.get_transformation_layout\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_transformation_layout\u001b[39m(\u001b[39mself\u001b[39m, model: NNCFNetwork) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PTTransformationLayout:\n\u001b[1;32m    131\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    Applies algorithm-specific modifications to the model. Hooks to be executed during model\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m    forward operation may be registered using NNCFNetwork command insertion methods. Additional\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m    :return: NNCFNetwork with algorithm-specific modifications applied\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     check_scopes_in_graph(\n\u001b[1;32m    140\u001b[0m         model\u001b[39m.\u001b[39;49mnncf\u001b[39m.\u001b[39;49mget_original_graph(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignored_scopes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_scopes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_scopes\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     layout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_transformation_layout(model)\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_frozen_layers(model)\n",
      "File \u001b[0;32m~/sixing/nncf_bootstrapnas/nncf/common/scopes.py:140\u001b[0m, in \u001b[0;36mcheck_scopes_in_graph\u001b[0;34m(graph, ignored_scopes, target_scopes, validate_scopes)\u001b[0m\n\u001b[1;32m    133\u001b[0m err_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    134\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRefer to the original_graph.dot to discover the operations \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39min the model currently visible to NNCF and specify the ignored/target \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mscopes in terms of the names there.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m validate_scopes:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(err_message)\n\u001b[1;32m    141\u001b[0m nncf_logger\u001b[39m.\u001b[39minfo(err_message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No match has been found among the model operations for the following ignored/target scope definitions:\n - ignored_scope: ['{re}.*qa_outputs.*']\nRefer to the original_graph.dot to discover the operations in the model currently visible to NNCF and specify the ignored/target scopes in terms of the names there."
     ]
    }
   ],
   "source": [
    "last_checkpoint = None\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    if nncf_config is not None:\n",
    "        train_result, model, elasticity_ctrl = train_result\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    if nncf_config is not None and training_args.do_search:\n",
    "        resuming_checkpoint_path = None\n",
    "        if resuming_checkpoint_path is None:\n",
    "            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)\n",
    "        else:\n",
    "            search_algo = SearchAlgorithm.from_checkpoint(model, elasticity_ctrl, None, resuming_checkpoint_path)\n",
    "\n",
    "        def validate_model_func(model_, dataset_):\n",
    "            #trainer.model will be used to evaluate(trainer.model = model)\n",
    "            metrics = trainer.evaluate(eval_dataset=dataset_)\n",
    "            return metrics['eval_f1']\n",
    "\n",
    "        elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,\n",
    "                                                                            eval_dataset,\n",
    "                                                                            training_args.output_dir)\n",
    "        logger.info(\"Best config: {best_config}\".format(best_config=best_config))\n",
    "        logger.info(\"Performance metrics: {performance_metrics}\".format(performance_metrics=performance_metrics))\n",
    "\n",
    "        search_algo.visualize_search_progression()\n",
    "        search_algo.search_progression_to_csv()\n",
    "        search_algo.evaluators_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nncf_huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
