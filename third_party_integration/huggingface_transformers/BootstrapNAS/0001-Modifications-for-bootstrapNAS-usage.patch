From 4065bcb9de2f6c91a670ae82b9c6cda5321c935e Mon Sep 17 00:00:00 2001
From: yzheng124 <yi.zheng@intel.com>
Date: Wed, 27 Jul 2022 01:42:51 -0700
Subject: [PATCH] Modifications for bootstrapNAS usage

---
 examples/pytorch/language-modeling/run_clm.py |  85 ++++++--
 examples/pytorch/question-answering/run_qa.py |  69 ++++++-
 .../pytorch/question-answering/trainer_qa.py  |   5 +-
 .../pytorch/text-classification/run_glue.py   |  84 ++++++--
 .../pytorch/text-classification/run_xnli.py   |  73 ++++++-
 .../pytorch/token-classification/run_ner.py   | 112 +++++++---
 .../nncf_bootstrapnas_bert_config_conll.json  |  63 ++++++
 .../nncf_bootstrapnas_bert_config_mrpc.json   |  67 ++++++
 .../nncf_bootstrapnas_bert_config_squad.json  |  68 +++++++
 .../nncf_bootstrapnas_bert_config_xnli.json   |  63 ++++++
 src/transformers/file_utils.py                |   2 +
 src/transformers/modeling_utils.py            |  23 ++-
 src/transformers/models/bert/modeling_bert.py |   6 +-
 .../models/mobilebert/modeling_mobilebert.py  |   2 +
 src/transformers/trainer.py                   | 191 ++++++++++++++++--
 src/transformers/trainer_callback.py          |  10 +
 src/transformers/training_args.py             |   9 +
 17 files changed, 847 insertions(+), 85 deletions(-)
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json
 create mode 100644 nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json

diff --git a/examples/pytorch/language-modeling/run_clm.py b/examples/pytorch/language-modeling/run_clm.py
index 0c002deeb..887f5c3f5 100755
--- a/examples/pytorch/language-modeling/run_clm.py
+++ b/examples/pytorch/language-modeling/run_clm.py
@@ -29,7 +29,11 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import onnx
+import torch
 from datasets import load_dataset
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 
 import transformers
 from transformers import (
@@ -49,7 +53,6 @@ from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
 
-
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
 
@@ -334,21 +337,7 @@ def main():
             "You can do it from another script, save it, and load it from here, using --tokenizer_name."
         )
 
-    if model_args.model_name_or_path:
-        model = AutoModelForCausalLM.from_pretrained(
-            model_args.model_name_or_path,
-            from_tf=bool(".ckpt" in model_args.model_name_or_path),
-            config=config,
-            cache_dir=model_args.cache_dir,
-            revision=model_args.model_revision,
-            use_auth_token=True if model_args.use_auth_token else None,
-        )
-    else:
-        model = AutoModelForCausalLM.from_config(config)
-        n_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())
-        logger.info(f"Training new model from scratch - Total size={n_params/2**20:.2f}M params")
 
-    model.resize_token_embeddings(len(tokenizer))
 
     # Preprocessing the datasets.
     # First we tokenize all the texts.
@@ -444,6 +433,47 @@ def main():
         if data_args.max_eval_samples is not None:
             eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    if model_args.model_name_or_path:
+        retval = AutoModelForCausalLM.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+            nncf_config=nncf_config,
+            nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+        )
+    else:
+        retval = AutoModelForCausalLM.from_config(config)
+        n_params = sum(dict((p.data_ptr(), p.numel()) for p in retval.parameters()).values())
+        logger.info(f"Training new model from scratch - Total size={n_params / 2 ** 20:.2f}M params")
+
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    model.resize_token_embeddings(len(tokenizer))
+
+    if training_args.to_onnx:
+        if nncf_config is not None:
+           compression_ctrl.export_model(training_args.to_onnx)
+        else:
+           model.to('cpu')
+           dummy_tensor = torch.ones([1, config.n_positions], dtype=torch.long)
+           onnx.export(model, dummy_tensor, training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -453,6 +483,7 @@ def main():
         tokenizer=tokenizer,
         # Data collator will default to DataCollatorWithPadding, so we change it.
         data_collator=default_data_collator,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
@@ -463,6 +494,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         trainer.save_model()  # Saves the tokenizer too for easy upload
 
         metrics = train_result.metrics
@@ -476,6 +509,28 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                try:
+                    perplexity = math.exp(metrics["eval_loss"])
+                except OverflowError:
+                    perplexity = float("inf")
+                return perplexity
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            search_algo.visualize_search_progression()
+            search_algo.search_progression_to_csv()
+            search_algo.evaluators_to_csv()
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index b8559bb72..425eb0c99 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -25,6 +25,7 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import torch
 from datasets import load_dataset, load_metric
 
 import transformers
@@ -46,6 +47,10 @@ from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
 from utils_qa import postprocess_qa_predictions
 
+from torch import onnx
+
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -298,14 +303,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForQuestionAnswering.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -555,6 +552,41 @@ def main():
     def compute_metrics(p: EvalPrediction):
         return metric.compute(predictions=p.predictions, references=p.label_ids)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForQuestionAnswering.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 384], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = QuestionAnsweringTrainer(
         model=model,
@@ -566,6 +598,7 @@ def main():
         data_collator=data_collator,
         post_process_function=post_processing_function,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
@@ -576,6 +609,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         trainer.save_model()  # Saves the tokenizer too for easy upload
 
         metrics = train_result.metrics
@@ -588,6 +623,24 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_f1']
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            search_algo.visualize_search_progression()
+            search_algo.search_progression_to_csv()
+            search_algo.evaluators_to_csv()
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/question-answering/trainer_qa.py b/examples/pytorch/question-answering/trainer_qa.py
index 7f98eba23..cf217268e 100644
--- a/examples/pytorch/question-answering/trainer_qa.py
+++ b/examples/pytorch/question-answering/trainer_qa.py
@@ -31,7 +31,7 @@ class QuestionAnsweringTrainer(Trainer):
         self.eval_examples = eval_examples
         self.post_process_function = post_process_function
 
-    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval"):
+    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = "eval", active_subnet=None):
         eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset
         eval_dataloader = self.get_eval_dataloader(eval_dataset)
         eval_examples = self.eval_examples if eval_examples is None else eval_examples
@@ -61,6 +61,9 @@ class QuestionAnsweringTrainer(Trainer):
                 if not key.startswith(f"{metric_key_prefix}_"):
                     metrics[f"{metric_key_prefix}_{key}"] = metrics.pop(key)
 
+            if active_subnet is not None:
+                metrics.update(active_subnet)
+
             self.log(metrics)
         else:
             metrics = {}
diff --git a/examples/pytorch/text-classification/run_glue.py b/examples/pytorch/text-classification/run_glue.py
index 6e66af923..1afa263e3 100755
--- a/examples/pytorch/text-classification/run_glue.py
+++ b/examples/pytorch/text-classification/run_glue.py
@@ -28,6 +28,9 @@ import numpy as np
 from datasets import load_dataset, load_metric
 
 import transformers
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
+
 from transformers import (
     AutoConfig,
     AutoModelForSequenceClassification,
@@ -327,14 +330,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Preprocessing the raw_datasets
     if data_args.task_name is not None:
@@ -360,12 +355,12 @@ def main():
     # Some models have set the order of the labels to use, so let's make sure we do use it.
     label_to_id = None
     if (
-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
+        config.label2id != PretrainedConfig(num_labels=num_labels).label2id
         and data_args.task_name is not None
         and not is_regression
     ):
         # Some have all caps in their config, some don't.
-        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
+        label_name_to_id = {k.lower(): v for k, v in config.label2id.items()}
         if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
             label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}
         else:
@@ -378,8 +373,8 @@ def main():
         label_to_id = {v: i for i, v in enumerate(label_list)}
 
     if label_to_id is not None:
-        model.config.label2id = label_to_id
-        model.config.id2label = {id: label for label, id in config.label2id.items()}
+        config.label2id = label_to_id
+        config.id2label = {id: label for label, id in config.label2id.items()}
 
     if data_args.max_seq_length > tokenizer.model_max_length:
         logger.warning(
@@ -414,6 +409,46 @@ def main():
         if data_args.max_train_samples is not None:
             train_dataset = train_dataset.select(range(data_args.max_train_samples))
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            import torch
+            from torch import onnx
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor),
+                        training_args.to_onnx, opset_version=10)
+
     if training_args.do_eval:
         if "validation" not in raw_datasets and "validation_matched" not in raw_datasets:
             raise ValueError("--do_eval requires a validation dataset")
@@ -471,8 +506,13 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+    if nncf_config is not None:
+        if not (training_args.local_rank == -1 or training_args.no_cuda):
+            compression_ctrl.distributed()
+
     # Training
     if training_args.do_train:
         checkpoint = None
@@ -481,6 +521,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         metrics = train_result.metrics
         max_train_samples = (
             data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
@@ -493,6 +535,24 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_accuracy'] * 100
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            search_algo.visualize_search_progression()
+            search_algo.search_progression_to_csv()
+            search_algo.evaluators_to_csv()
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/text-classification/run_xnli.py b/examples/pytorch/text-classification/run_xnli.py
index d7e5dfd83..6fc24ea40 100755
--- a/examples/pytorch/text-classification/run_xnli.py
+++ b/examples/pytorch/text-classification/run_xnli.py
@@ -26,7 +26,10 @@ from typing import Optional
 
 import datasets
 import numpy as np
+import torch
 from datasets import load_dataset, load_metric
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
 
 import transformers
 from transformers import (
@@ -250,14 +253,6 @@ def main():
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
     )
-    model = AutoModelForSequenceClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
 
     # Preprocessing the datasets
     # Padding strategy
@@ -331,6 +326,43 @@ def main():
     else:
         data_collator = None
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForSequenceClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+    if training_args.to_onnx:
+        # Expecting the following forward signature:
+        # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, training_args.max_seq_length], dtype=torch.long)
+            torch.onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx)
+
     # Initialize our Trainer
     trainer = Trainer(
         model=model,
@@ -340,8 +372,13 @@ def main():
         compute_metrics=compute_metrics,
         tokenizer=tokenizer,
         data_collator=data_collator,
+        compression_ctrl=compression_ctrl
     )
 
+    if nncf_config is not None:
+        if not (training_args.local_rank == -1 or training_args.no_cuda):
+            compression_ctrl.distributed()
+
     # Training
     if training_args.do_train:
         checkpoint = None
@@ -350,6 +387,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         metrics = train_result.metrics
         max_train_samples = (
             data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
@@ -362,6 +401,24 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_accuracy'] * 100
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            search_algo.visualize_search_progression()
+            search_algo.search_progression_to_csv()
+            search_algo.evaluators_to_csv()
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/examples/pytorch/token-classification/run_ner.py b/examples/pytorch/token-classification/run_ner.py
index 65c26cd9e..a635bd8a1 100755
--- a/examples/pytorch/token-classification/run_ner.py
+++ b/examples/pytorch/token-classification/run_ner.py
@@ -22,30 +22,35 @@ Fine-tuning the library models for token classification.
 import logging
 import os
 import sys
-from dataclasses import dataclass, field
+from dataclasses import dataclass
+from dataclasses import field
+from typing import List
 from typing import Optional
 
 import datasets
 import numpy as np
-from datasets import ClassLabel, load_dataset, load_metric
-
+import torch
 import transformers
-from transformers import (
-    AutoConfig,
-    AutoModelForTokenClassification,
-    AutoTokenizer,
-    DataCollatorForTokenClassification,
-    HfArgumentParser,
-    PreTrainedTokenizerFast,
-    Trainer,
-    TrainingArguments,
-    set_seed,
-)
+from datasets import ClassLabel
+from datasets import load_dataset
+from datasets import load_metric
+from nncf import NNCFConfig
+from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm
+from packaging import version
+from torch import onnx
+from transformers import AutoConfig
+from transformers import AutoModelForTokenClassification
+from transformers import AutoTokenizer
+from transformers import DataCollatorForTokenClassification
+from transformers import HfArgumentParser
+from transformers import PreTrainedTokenizerFast
+from transformers import Trainer
+from transformers import TrainingArguments
+from transformers import set_seed
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
 
-
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
 
@@ -177,6 +182,16 @@ class DataTrainingArguments:
         self.task_name = self.task_name.lower()
 
 
+def filter_columns(dataset, keep_columns: List[str], remove_columns: List[str]):
+    if version.parse(datasets.__version__) < version.parse("1.4.0"):
+        dataset.set_format(
+            type=dataset.format["type"], columns=keep_columns, format_kwargs=dataset.format["format_kwargs"]
+        )
+        return dataset
+    else:
+        return dataset.remove_columns(remove_columns)
+
+
 def main():
     # See all possible arguments in src/transformers/training_args.py
     # or by passing the --help flag to this script.
@@ -331,14 +346,7 @@ def main():
             use_auth_token=True if model_args.use_auth_token else None,
         )
 
-    model = AutoModelForTokenClassification.from_pretrained(
-        model_args.model_name_or_path,
-        from_tf=bool(".ckpt" in model_args.model_name_or_path),
-        config=config,
-        cache_dir=model_args.cache_dir,
-        revision=model_args.model_revision,
-        use_auth_token=True if model_args.use_auth_token else None,
-    )
+
 
     # Tokenizer check: this script requires a fast tokenizer.
     if not isinstance(tokenizer, PreTrainedTokenizerFast):
@@ -432,6 +440,43 @@ def main():
     # Data collator
     data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)
 
+    nncf_config = None
+    if training_args.nncf_config is not None:
+        nncf_config = NNCFConfig.from_json(training_args.nncf_config)
+        if nncf_config.get("log_dir") is None:
+            nncf_config["log_dir"] = training_args.output_dir
+        if not os.path.exists(training_args.output_dir) and training_args.local_rank in [-1, 0]:
+            os.makedirs(nncf_config["log_dir"])
+
+    retval = AutoModelForTokenClassification.from_pretrained(
+        model_args.model_name_or_path,
+        from_tf=bool(".ckpt" in model_args.model_name_or_path),
+        config=config,
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+        nncf_config=nncf_config,
+        nncf_eval=nncf_config is not None and training_args.do_eval and not training_args.do_train
+    )
+
+    if nncf_config is None:
+        model = retval
+        compression_ctrl = None
+    else:
+        compression_ctrl, model = retval
+
+
+    if training_args.to_onnx:
+    # Expecting the following forward signature:
+    # (input_ids, attention_mask, token_type_ids, ...)
+        if nncf_config is not None:
+            compression_ctrl.export_model(training_args.to_onnx)
+        else:
+            model.to('cpu')
+            dummy_tensor = torch.ones([1, 128], dtype=torch.long)
+            onnx.export(model, (dummy_tensor, dummy_tensor, dummy_tensor), training_args.to_onnx,
+                        opset_version=10)
+
     # Metrics
     metric = load_metric("seqeval")
 
@@ -477,6 +522,7 @@ def main():
         tokenizer=tokenizer,
         data_collator=data_collator,
         compute_metrics=compute_metrics,
+        compression_ctrl=compression_ctrl
     )
 
     # Training
@@ -487,6 +533,8 @@ def main():
         elif last_checkpoint is not None:
             checkpoint = last_checkpoint
         train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        if nncf_config is not None:
+            train_result, model, elasticity_ctrl = train_result
         metrics = train_result.metrics
         trainer.save_model()  # Saves the tokenizer too for easy upload
 
@@ -499,6 +547,24 @@ def main():
         trainer.save_metrics("train", metrics)
         trainer.save_state()
 
+        if nncf_config is not None and training_args.do_search:
+            search_algo = SearchAlgorithm.from_config(model, elasticity_ctrl, nncf_config)
+
+            def validate_model_func(model_, dataset_):
+                #trainer.model will be used to evaluate(trainer.model = model)
+                metrics = trainer.evaluate(eval_dataset=dataset_)
+                return metrics['eval_accuracy'] * 100
+
+            elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_func,
+                                                                                 eval_dataset,
+                                                                                 training_args.output_dir)
+            logger.info("Best config: {best_config}".format(best_config=best_config))
+            logger.info("Performance metrics: {performance_metrics}".format(performance_metrics=performance_metrics))
+
+            search_algo.visualize_search_progression()
+            search_algo.search_progression_to_csv()
+            search_algo.evaluators_to_csv()
+
     # Evaluation
     if training_args.do_eval:
         logger.info("*** Evaluate ***")
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json
new file mode 100644
index 000000000..9207e4bb2
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_conll.json
@@ -0,0 +1,63 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 3, "depth_indicator": 2, "width_indicator": 3, "init_lr": 5e-5, "epochs_lr": 3, "sample_rate": 10},
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                      "overwrite_groups": [
+                        ["BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                         "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                         "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"]
+                      ],
+                      "overwrite_groups_widths": [[3072, 2912, 2760]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/__add___0"
+                        ],
+                        [
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForTokenClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/__add___0"
+                        ]
+                    ],
+                }
+            },
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 5,
+            "ref_acc": 99.17
+        }
+    }
+}
\ No newline at end of file
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json
new file mode 100644
index 000000000..aa6a5c1a5
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_mrpc.json
@@ -0,0 +1,67 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 5, "depth_indicator": 2,  "width_indicator": 3, "init_lr": 5e-5, "epochs_lr": 5},
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "max_num_widths": 3,
+                      "min_out_channels": 32,
+                      "width_step": 32,
+                      "width_multipliers": [1, 0.80, 0.65],
+                      "overwrite_groups": [
+                        ["BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                         "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                         "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"]
+                      ],
+                      "overwrite_groups_widths": [[3072, 2456, 1992]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/__add___0"
+                        ],
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/__add___0"
+                        ]
+                    ],
+                }
+            },
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 5,
+            "ref_acc": 84.56
+        }
+    }
+}
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json
new file mode 100644
index 000000000..bb2b1d2c1
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_squad.json
@@ -0,0 +1,68 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 384],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 2, "depth_indicator": 2, "width_indicator": 3, "init_lr": 3e-5, "epochs_lr": 2},
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "overwrite_groups": [
+                        [
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[3]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[4]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[5]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"
+                        ]
+                    ],
+                    "overwrite_groups_widths": [[4096, 3888, 3680]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[12]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0"
+                        ],
+                        [
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[12]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForQuestionAnswering/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[13]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0"
+                        ]
+                    ]
+                }
+            },
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 5,
+            "ref_acc": 93.21
+        }
+    }
+}
diff --git a/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json
new file mode 100644
index 000000000..b49bb74c4
--- /dev/null
+++ b/nncf_bootstrapnas_config/nncf_bootstrapnas_bert_config_xnli.json
@@ -0,0 +1,63 @@
+{
+    "input_info": [
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        },
+        {
+            "sample_size": [1, 128],
+            "type": "long"
+        }
+    ],
+    "bootstrapNAS": {
+        "training": {
+            "algorithm": "progressive_shrinking",
+            "progressivity_of_elasticity": ["depth", "width"],
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "schedule": {
+                "list_stage_descriptions": [
+                    {"train_dims": ["depth", "width"], "epochs": 2, "depth_indicator": 2, "width_indicator": 3, "init_lr": 5e-5, "epochs_lr": 2},
+                ]
+            },
+            "elasticity": {
+                "available_elasticity_dims": ["depth", "width"],
+                "width": {
+                    "overwrite_groups": [
+                        ["BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[0]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                         "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[1]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0",
+                         "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[2]/BertIntermediate[intermediate]/NNCFLinear[dense]/linear_0"]
+                    ],
+                    "overwrite_groups_widths" : [[3072, 2912, 2760]]
+                },
+                "depth": {
+                    "mode": "manual",
+                    "skipped_blocks": [
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[9]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/__add___0"
+                        ],
+                        [
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[10]/BertOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0",
+                            "BertForSequenceClassification/BertModel[bert]/BertEncoder[encoder]/ModuleList[layer]/BertLayer[11]/BertOutput[output]/__add___0"
+                        ]
+                    ],
+                }
+            },
+        },
+        "search": {
+            "algorithm": "NSGA2",
+            "batchnorm_adaptation": {
+                "num_bn_adaptation_samples": 0
+            },
+            "num_evals": 20,
+            "population": 5,
+            "ref_acc": 77.68
+        }
+    }
+}
diff --git a/src/transformers/file_utils.py b/src/transformers/file_utils.py
index bc2b29354..bc2ffa90a 100644
--- a/src/transformers/file_utils.py
+++ b/src/transformers/file_utils.py
@@ -244,10 +244,12 @@ SESSION_ID = uuid4().hex
 DISABLE_TELEMETRY = os.getenv("DISABLE_TELEMETRY", False) in ENV_VARS_TRUE_VALUES
 
 WEIGHTS_NAME = "pytorch_model.bin"
+NNCF_PT_STATE_NAME = "nncf_state.bin"
 TF2_WEIGHTS_NAME = "tf_model.h5"
 TF_WEIGHTS_NAME = "model.ckpt"
 FLAX_WEIGHTS_NAME = "flax_model.msgpack"
 CONFIG_NAME = "config.json"
+NNCF_CONFIG_NAME = 'nncf_config.json'
 FEATURE_EXTRACTOR_NAME = "preprocessor_config.json"
 MODEL_CARD_NAME = "modelcard.json"
 
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index b34637b02..501e14cf6 100644
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -25,6 +25,9 @@ from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 import torch
 from torch import Tensor, device, nn
 from torch.nn import CrossEntropyLoss
+from nncf.torch.model_creation import create_nncf_network
+from nncf.experimental.torch.nas.bootstrapNAS.training.model_creator_helpers import \
+    create_compressed_model_from_algo_names
 
 from .activations import get_activation
 from .configuration_utils import PretrainedConfig
@@ -923,6 +926,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         state_dict: Optional[dict] = None,
         save_function: Callable = torch.save,
         push_to_hub: bool = False,
+        nncf_compression_state: Dict = None,
         **kwargs,
     ):
         """
@@ -1166,6 +1170,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         from_auto_class = kwargs.pop("_from_auto", False)
         _fast_init = kwargs.pop("_fast_init", True)
         torch_dtype = kwargs.pop("torch_dtype", None)
+        nncf_config = kwargs.pop("nncf_config", None)
+        nncf_eval = kwargs.pop("nncf_eval", False)
 
         from_pt = not (from_tf | from_flax)
 
@@ -1349,6 +1355,13 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 )
                 raise
         elif from_pt:
+            if nncf_config is not None and nncf_eval:
+                nncf_network = create_nncf_network(model, nncf_config)
+                algo_name = nncf_config.get('bootstrapNAS', {}).get('training', {}).get('algorithm', 'progressive_shrinking')
+                compression_ctrl, model = create_compressed_model_from_algo_names(nncf_network, nncf_config,
+                                                                               algo_names=[algo_name])
+                return compression_ctrl, model
+
             model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_state_dict_into_model(
                 model,
                 state_dict,
@@ -1363,6 +1376,13 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         # Set model in evaluation mode to deactivate DropOut modules by default
         model.eval()
 
+        if nncf_config is not None:
+            nncf_network = create_nncf_network(model, nncf_config)
+            algo_name = nncf_config.get('bootstrapNAS', {}).get('training', {}).get('algorithm', 'progressive_shrinking')
+            compression_ctrl, model = create_compressed_model_from_algo_names(nncf_network, nncf_config,
+                                                                            algo_names=[algo_name])
+            return compression_ctrl, model
+
         if output_loading_info:
             loading_info = {
                 "missing_keys": missing_keys,
@@ -1562,7 +1582,8 @@ PreTrainedModel.push_to_hub.__doc__ = PreTrainedModel.push_to_hub.__doc__.format
     object="model", object_class="AutoModel", object_files="model checkpoint"
 )
 
-
+import nncf
+@nncf.torch.register_module()
 class Conv1D(nn.Module):
     """
     1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).
diff --git a/src/transformers/models/bert/modeling_bert.py b/src/transformers/models/bert/modeling_bert.py
index 9606af376..ccf86bc66 100755
--- a/src/transformers/models/bert/modeling_bert.py
+++ b/src/transformers/models/bert/modeling_bert.py
@@ -250,7 +250,7 @@ class BertSelfAttention(nn.Module):
         self.is_decoder = config.is_decoder
 
     def transpose_for_scores(self, x):
-        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, -1) #self.attention_head_size)
         x = x.view(*new_x_shape)
         return x.permute(0, 2, 1, 3)
 
@@ -339,7 +339,7 @@ class BertSelfAttention(nn.Module):
         context_layer = torch.matmul(attention_probs, value_layer)
 
         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
+        new_context_layer_shape = context_layer.size()[:-2] + (-1,)#(self.all_head_size,)
         context_layer = context_layer.view(*new_context_layer_shape)
 
         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
@@ -630,7 +630,7 @@ class BertPooler(nn.Module):
     def forward(self, hidden_states):
         # We "pool" the model by simply taking the hidden state corresponding
         # to the first token.
-        first_token_tensor = hidden_states[:, 0]
+        first_token_tensor = torch.chunk(hidden_states, hidden_states.shape[1], dim=1)[0].squeeze(1)
         pooled_output = self.dense(first_token_tensor)
         pooled_output = self.activation(pooled_output)
         return pooled_output
diff --git a/src/transformers/models/mobilebert/modeling_mobilebert.py b/src/transformers/models/mobilebert/modeling_mobilebert.py
index 448a894be..0ed1d221b 100644
--- a/src/transformers/models/mobilebert/modeling_mobilebert.py
+++ b/src/transformers/models/mobilebert/modeling_mobilebert.py
@@ -140,6 +140,8 @@ def load_tf_weights_in_mobilebert(model, config, tf_checkpoint_path):
     return model
 
 
+import nncf
+@nncf.torch.register_module()
 class NoNorm(nn.Module):
     def __init__(self, feat_size, eps=None):
         super().__init__()
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index d6e0d51c4..e4092823a 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -17,6 +17,7 @@ The Trainer class, to easily train a  Transformers from scratch or finetune
 """
 
 import collections
+from functools import partial
 import inspect
 import math
 import os
@@ -30,6 +31,7 @@ from logging import StreamHandler
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
 
+from nncf.torch.nncf_network import NNCFNetwork
 from tqdm.auto import tqdm
 
 
@@ -47,6 +49,9 @@ from .integrations import (  # isort: split
 
 import numpy as np
 import torch
+from nncf.api.compression import CompressionStage
+from nncf.torch.compression_method_api import PTCompressionAlgorithmController
+from nncf.common.utils.tensorboard import prepare_for_tensorboard
 from packaging import version
 from torch import nn
 from torch.utils.data.dataloader import DataLoader
@@ -62,6 +67,7 @@ from .deepspeed import deepspeed_init, is_deepspeed_zero3_enabled
 from .dependency_versions_check import dep_version_check
 from .file_utils import (
     CONFIG_NAME,
+    NNCF_CONFIG_NAME,
     WEIGHTS_NAME,
     PushToHubMixin,
     is_apex_available,
@@ -71,9 +77,10 @@ from .file_utils import (
     is_sagemaker_mp_enabled,
     is_torch_tpu_available,
     is_training_run_on_sagemaker,
+    replace_return_docstrings,
 )
 from .modelcard import TrainingSummary
-from .modeling_utils import PreTrainedModel, unwrap_model
+from .modeling_utils import PreTrainedModel, no_init_weights, unwrap_model
 from .optimization import Adafactor, AdamW, get_scheduler
 from .tokenization_utils_base import PreTrainedTokenizerBase
 from .trainer_callback import (
@@ -274,12 +281,15 @@ class Trainer:
         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
         callbacks: Optional[List[TrainerCallback]] = None,
         optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
+        compression_ctrl: PTCompressionAlgorithmController = None
     ):
         if args is None:
             output_dir = "tmp_trainer"
             logger.info(f"No `TrainingArguments` passed, using `output_dir={output_dir}`.")
             args = TrainingArguments(output_dir=output_dir)
         self.args = args
+
+        self.compression_ctrl = compression_ctrl
         # Seed must be set before instantiating the model when using model
         set_seed(self.args.seed)
         self.hp_name = None
@@ -510,7 +520,10 @@ class Trainer:
             return dataset
         if self._signature_columns is None:
             # Inspect model forward signature to keep only the arguments it accepts.
-            signature = inspect.signature(self.model.forward)
+            if isinstance(self.model, NNCFNetwork):
+                signature = inspect.signature(self.model.get_nncf_wrapped_model().forward)
+            else:
+                signature = inspect.signature(self.model.forward)
             self._signature_columns = list(signature.parameters.keys())
             # Labels may be named label or label_ids, the default data collator handles that.
             self._signature_columns += ["label", "label_ids"]
@@ -829,6 +842,25 @@ class Trainer:
                 num_training_steps=num_training_steps,
             )
 
+    def update_lr_scheduler_func(self):
+        global state_dict, load_state_dict
+
+        def state_dict(scheduler) -> Dict[str, Any]:
+            """
+            Returns the state of the scheduler as a :class:`dict`.
+            """
+            return {key: value for key, value in scheduler.__dict__.items() if key != '_optimizer'}
+
+        def load_state_dict(scheduler, state_dict: Dict[str, Any]):
+            """
+            Loads the schedulers state.
+            :param state_dict (dict): scheduler state.
+            """
+            scheduler.__dict__.update(state_dict)
+
+        self.lr_scheduler.state_dict = partial(state_dict, scheduler=self.lr_scheduler)
+        self.lr_scheduler.load_state_dict = partial(load_state_dict, scheduler=self.lr_scheduler)
+
     def num_examples(self, dataloader: DataLoader) -> int:
         """
         Helper to get number of samples in a :class:`~torch.utils.data.DataLoader` by accessing its dataset.
@@ -899,6 +931,9 @@ class Trainer:
                 self.state.save_to_json(os.path.join(output_dir, "trainer_state.json"))
                 torch.save(self.optimizer.state_dict(), os.path.join(output_dir, "optimizer.pt"))
                 torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, "scheduler.pt"))
+                if self.compression_ctrl is not None:
+                    elasticity_state = self.compression_ctrl.elasticity_controller.get_compression_state()
+                    torch.save(elasticity_state, os.path.join(output_dir, "elasticity_state.pt"))
 
     def call_model_init(self, trial=None):
         model_init_argcount = number_of_arguments(self.model_init)
@@ -972,6 +1007,9 @@ class Trainer:
                 find_unused_parameters = not getattr(model.config, "gradient_checkpointing", False)
             else:
                 find_unused_parameters = True
+
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.distributed()
             model = nn.parallel.DistributedDataParallel(
                 model,
                 device_ids=[self.args.local_rank],
@@ -1091,7 +1129,26 @@ class Trainer:
         # number of training steps per epoch: num_update_steps_per_epoch
         # total number of training steps to execute: max_steps
         total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size
-        if train_dataset_is_sized:
+        if self.compression_ctrl is not None:
+            #save nncf_config
+            nncf_config_save_path = os.path.join(args.output_dir, NNCF_CONFIG_NAME)
+            shutil.copyfile(args.nncf_config, nncf_config_save_path)
+
+            self.best_compression_stage = CompressionStage.UNCOMPRESSED
+
+            assert train_dataset_is_sized, "dataset has no __len__"
+            num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps
+            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
+            self.create_optimizer()
+            #use our own lr scheduler
+            self.compression_ctrl.set_training_lr_scheduler_args(self.optimizer, num_update_steps_per_epoch)
+            self.lr_scheduler = self.compression_ctrl.scheduler.lr_scheduler
+            self.update_lr_scheduler_func()
+
+            num_train_epochs = self.compression_ctrl.get_total_num_epochs()
+            max_steps = math.ceil(num_train_epochs * num_update_steps_per_epoch)
+            num_train_samples = len(self.train_dataset) * num_train_epochs
+        elif train_dataset_is_sized:
             num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps
             num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
             if args.max_steps > 0:
@@ -1231,11 +1288,26 @@ class Trainer:
                     break
 
         for epoch in range(epochs_trained, num_train_epochs):
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.scheduler.epoch_step()
+                self.compression_stage = self.compression_ctrl.compression_stage()
+
+                def get_search_space(compression_ctrl):
+                    m_handler = compression_ctrl.multi_elasticity_handler
+                    active_handlers = {
+                        dim: m_handler._handlers[dim] for dim in m_handler._handlers if m_handler._is_handler_enabled_map[dim]
+                    }
+                    space = {}
+                    for handler_id, handler in active_handlers.items():
+                        space[handler_id.value] = handler.get_search_space()
+                    return space
+
+                logger.info(get_search_space(self.compression_ctrl))
+                logger.info(self.compression_ctrl.statistics().to_str())
             if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):
                 train_dataloader.sampler.set_epoch(epoch)
             elif isinstance(train_dataloader.dataset, IterableDatasetShard):
                 train_dataloader.dataset.set_epoch(epoch)
-
             if is_torch_tpu_available():
                 parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)
                 epoch_iterator = parallel_loader
@@ -1275,9 +1347,11 @@ class Trainer:
                 ):
                     # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.
                     with model.no_sync():
-                        tr_loss += self.training_step(model, inputs)
+                        curr_loss = self.training_step(model, inputs)
                 else:
-                    tr_loss += self.training_step(model, inputs)
+                    curr_loss = self.training_step(model, inputs)
+
+                tr_loss += curr_loss
                 self.current_flos += float(self.floating_point_ops(inputs))
 
                 # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps
@@ -1311,6 +1385,8 @@ class Trainer:
                             )
 
                     # Optimizer step
+                    if self.compression_ctrl is not None:
+                        self.compression_ctrl.step()
                     optimizer_was_run = True
                     if self.deepspeed:
                         pass  # called outside the loop
@@ -1328,9 +1404,10 @@ class Trainer:
                     if optimizer_was_run and not self.deepspeed:
                         self.lr_scheduler.step()
 
-                    model.zero_grad()
+                    model.zero_grad(set_to_none=True)
                     self.state.global_step += 1
                     self.state.epoch = epoch + (step + 1) / steps_in_epoch
+                    self.state.curr_loss = curr_loss.cpu().detach().item()
                     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
 
                     self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
@@ -1340,6 +1417,8 @@ class Trainer:
 
             self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)
             self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
+            if self.compression_ctrl is not None:
+                self.best_compression_stage = max(self.compression_stage, self.best_compression_stage)
 
             if DebugOption.TPU_METRICS_DEBUG in self.args.debug:
                 if is_torch_tpu_available():
@@ -1403,6 +1482,9 @@ class Trainer:
 
         self.control = self.callback_handler.on_train_end(args, self.state, self.control)
 
+        if self.compression_ctrl is not None:
+            return TrainOutput(self.state.global_step, train_loss, metrics), model, self.compression_ctrl.elasticity_controller
+
         return TrainOutput(self.state.global_step, train_loss, metrics)
 
     def _load_state_dict_in_model(self, state_dict):
@@ -1426,6 +1508,13 @@ class Trainer:
             logs["loss"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)
             logs["learning_rate"] = self._get_learning_rate()
 
+            if self.compression_ctrl is not None:
+                logs["compression_loss"] = self.compression_ctrl.loss().item()
+                compression_stats = self.compression_ctrl.statistics()
+                for key, value in prepare_for_tensorboard(compression_stats).items():
+                    logs["compression/statistics/{0}".format(key)] = value
+                print(compression_stats.to_str())
+
             self._total_loss_scalar += tr_loss_scalar
             self._globalstep_last_logged = self.state.global_step
             self.store_flos()
@@ -1433,12 +1522,28 @@ class Trainer:
             self.log(logs)
 
         metrics = None
+        metrics_minsubnet = None
+        metrics_supernet = None
         if self.control.should_evaluate:
-            metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
-            self._report_to_hp_search(trial, epoch, metrics)
+            if self.compression_ctrl is not None:
+                self.compression_ctrl.multi_elasticity_handler.activate_minimum_subnet()
+                active_subnet = self.compression_ctrl.multi_elasticity_handler.get_active_config()
+                logger.info(f'Minimum SubNet={active_subnet}')
+                metrics_minsubnet = self.evaluate(ignore_keys=ignore_keys_for_eval, active_subnet={'Minimum SubNet': str(active_subnet)})
+                self._report_to_hp_search(trial, epoch, metrics_minsubnet)
+
+                self.compression_ctrl.multi_elasticity_handler.activate_supernet()
+                active_subnet = self.compression_ctrl.multi_elasticity_handler.get_active_config()
+                logger.info(f'SuperNet={active_subnet}')
+                metrics_supernet = self.evaluate(ignore_keys=ignore_keys_for_eval, active_subnet={'SuperNet': str(active_subnet)})
+                self._report_to_hp_search(trial, epoch, metrics_supernet)
+            else:
+                metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
+                self._report_to_hp_search(trial, epoch, metrics)
 
         if self.control.should_save:
-            self._save_checkpoint(model, trial, metrics=metrics)
+            self._save_checkpoint(model, trial, metrics=metrics, metrics_minsubnet=metrics_minsubnet,
+                                  metrics_supernet=metrics_supernet)
             self.control = self.callback_handler.on_save(self.args, self.state, self.control)
 
     def _load_rng_state(self, checkpoint):
@@ -1476,7 +1581,13 @@ class Trainer:
         if is_torch_tpu_available():
             xm.set_rng_state(checkpoint_rng_state["xla"])
 
-    def _save_checkpoint(self, model, trial, metrics=None):
+    def is_boostrapNAS_best_accuracy(self, metric_value, best_metric_value, operator,
+                                         compression_stage, best_compression_stage):
+        is_best_by_accuracy = operator(metric_value, best_metric_value) and compression_stage == best_compression_stage
+        is_best = is_best_by_accuracy or compression_stage > best_compression_stage
+        return is_best
+
+    def _save_checkpoint(self, model, trial, metrics=None,  metrics_minsubnet=None, metrics_supernet=None):
         # In all cases, including ddp/dp/deepspeed, self.model is always a reference to the model we
         # want to save except FullyShardedDDP.
         # assert unwrap_model(model) is self.model, "internal model should be a reference to self.model"
@@ -1551,9 +1662,42 @@ class Trainer:
                 self.state.best_metric = metric_value
                 self.state.best_model_checkpoint = output_dir
 
+        #boostrapNAS state
+        if (
+            self.compression_ctrl is not None and self.args.metric_for_best_model is not None
+            and metrics_supernet is not None and metrics_minsubnet is not None
+        ):
+            metric_to_check = self.args.metric_for_best_model
+            if not metric_to_check.startswith("eval_"):
+                metric_to_check = f"eval_{metric_to_check}"
+            metric_supernet_value = metrics_supernet[metric_to_check]
+            metrics_minsubnet_value = metrics_minsubnet[metric_to_check]
+
+            self.state.supernet_acc = metric_supernet_value
+            self.state.min_subnet_acc = metrics_minsubnet_value
+
+            operator = np.greater if self.args.greater_is_better else np.less
+            if(
+                self.state.supernet_best_acc is None
+                or self.state.best_supernet_model_checkpoint is None
+                or self.is_boostrapNAS_best_accuracy(metric_supernet_value, self.state.supernet_best_acc, operator,
+                                                          self.compression_stage, self.best_compression_stage)
+            ):
+                self.state.supernet_best_acc = metric_supernet_value
+                self.state.best_supernet_model_checkpoint = output_dir
+            if(
+                self.state.min_subnet_best_acc is None
+                or self.is_boostrapNAS_best_accuracy(metrics_minsubnet_value, self.state.min_subnet_best_acc, operator,
+                                                          self.compression_stage, self.best_compression_stage)
+            ):
+                self.state.min_subnet_best_acc = metrics_minsubnet_value
+
         # Save the Trainer state
         if self.args.should_save:
             self.state.save_to_json(os.path.join(output_dir, "trainer_state.json"))
+            if self.compression_ctrl is not None:
+                elasticity_state = self.compression_ctrl.elasticity_controller.get_compression_state()
+                torch.save(elasticity_state, os.path.join(output_dir, "elasticity_state.pt"))
 
         # Save RNG state in non-distributed training
         rng_states = {
@@ -1779,6 +1923,10 @@ class Trainer:
             # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`
             loss = loss / self.args.gradient_accumulation_steps
 
+        if self.compression_ctrl is not None:
+            compression_loss = self.compression_ctrl.loss()
+            loss += compression_loss
+
         if self.use_amp:
             self.scaler.scale(loss).backward()
         elif self.use_apex:
@@ -1917,13 +2065,20 @@ class Trainer:
         output_dir = output_dir if output_dir is not None else self.args.output_dir
         os.makedirs(output_dir, exist_ok=True)
         logger.info(f"Saving model checkpoint to {output_dir}")
+
+
         # Save a trained model and configuration using `save_pretrained()`.
         # They can then be reloaded using `from_pretrained()`
         if not isinstance(self.model, PreTrainedModel):
-            if isinstance(unwrap_model(self.model), PreTrainedModel):
+            unwrapped_model = unwrap_model(self.model)
+            if isinstance(unwrapped_model, NNCFNetwork):
+                is_pretrained = isinstance(unwrapped_model.get_nncf_wrapped_model(), PreTrainedModel)
+            else:
+                is_pretrained = isinstance(unwrapped_model, PreTrainedModel)
+            if is_pretrained:
                 if state_dict is None:
-                    state_dict = self.model.state_dict()
-                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)
+                    state_dict = unwrapped_model.state_dict()
+                unwrapped_model.save_pretrained(output_dir, state_dict=state_dict)
             else:
                 logger.info("Trainer.model is not a `PreTrainedModel`, only saving its state dict.")
                 if state_dict is None:
@@ -1934,6 +2089,10 @@ class Trainer:
         if self.tokenizer is not None:
             self.tokenizer.save_pretrained(output_dir)
 
+        if self.compression_ctrl is not None:
+            elasticity_state = self.compression_ctrl.elasticity_controller.get_compression_state()
+            torch.save(elasticity_state, os.path.join(output_dir, "elasticity_state.pt"))
+
         # Good practice: save your training arguments together with the trained model
         torch.save(self.args, os.path.join(output_dir, "training_args.bin"))
 
@@ -2000,6 +2159,7 @@ class Trainer:
         eval_dataset: Optional[Dataset] = None,
         ignore_keys: Optional[List[str]] = None,
         metric_key_prefix: str = "eval",
+        active_subnet: Dict[str, str] = None
     ) -> Dict[str, float]:
         """
         Run evaluation and returns metrics.
@@ -2042,6 +2202,9 @@ class Trainer:
             metric_key_prefix=metric_key_prefix,
         )
 
+        if active_subnet is not None:
+            output.metrics.update(active_subnet)
+
         total_batch_size = self.args.eval_batch_size * self.args.world_size
         output.metrics.update(
             speed_metrics(
diff --git a/src/transformers/trainer_callback.py b/src/transformers/trainer_callback.py
index 023418b3f..eec975aa6 100644
--- a/src/transformers/trainer_callback.py
+++ b/src/transformers/trainer_callback.py
@@ -23,6 +23,7 @@ from typing import Dict, List, Optional, Union
 
 import numpy as np
 from tqdm.auto import tqdm
+from nncf.api.compression import CompressionStage
 
 from .trainer_utils import IntervalStrategy
 from .training_args import TrainingArguments
@@ -87,6 +88,13 @@ class TrainerState:
     trial_name: str = None
     trial_params: Dict[str, Union[str, float, int, bool]] = None
 
+    #boostrapNAS
+    supernet_best_acc: float = None
+    supernet_acc: float = None
+    min_subnet_best_acc: float = None
+    min_subnet_acc: float = None
+    best_supernet_model_checkpoint: Optional[str] = None
+
     def __post_init__(self):
         if self.log_history is None:
             self.log_history = []
@@ -459,6 +467,8 @@ class ProgressCallback(TrainerCallback):
     def on_step_end(self, args, state, control, **kwargs):
         if state.is_local_process_zero:
             self.training_bar.update(state.global_step - self.current_step)
+            if hasattr(state, "curr_loss"):
+                self.training_bar.set_postfix(loss=state.curr_loss)
             self.current_step = state.global_step
 
     def on_prediction_step(self, args, state, control, eval_dataloader=None, **kwargs):
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 404d92a22..45d3d55b2 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -86,6 +86,8 @@ class TrainingArguments:
             Whether to run training or not. This argument is not directly used by :class:`~transformers.Trainer`, it's
             intended to be used by your training/evaluation scripts instead. See the `example scripts
             <https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.
+        do_search (:obj:`bool`, `optional`, defaults to :obj:`False`):
+            Whether to run boostrapNAS searching or not.
         do_eval (:obj:`bool`, `optional`):
             Whether to run evaluation on the validation set or not. Will be set to :obj:`True` if
             :obj:`evaluation_strategy` is different from :obj:`"no"`. This argument is not directly used by
@@ -359,6 +361,7 @@ class TrainingArguments:
     )
 
     do_train: bool = field(default=False, metadata={"help": "Whether to run training."})
+    do_search: bool = field(default=False, metadata={"help": "Whether to run boostrapNAS search."})
     do_eval: bool = field(default=False, metadata={"help": "Whether to run eval on the dev set."})
     do_predict: bool = field(default=False, metadata={"help": "Whether to run predictions on the test set."})
     evaluation_strategy: IntervalStrategy = field(
@@ -625,6 +628,12 @@ class TrainingArguments:
         metadata={"help": "Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer"},
     )
 
+    nncf_config: str = field(default=None,
+                             metadata={"help": "NNCF configuration .json file for compression-enabled training"})
+
+    to_onnx: str = field(default=None,
+                         metadata={"help": "Name of the ONNX model file to export the model to."})
+
     def __post_init__(self):
         # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
         # This needs to happen before any call to self.device or self.n_gpu.
-- 
2.17.1

